---
title: "Streaming Responses"
slug: "v2/docs/streaming"

hidden: false
description: "The document explains how the Chat API can stream events like text generation in real-time, allowing for partial results to be displayed quickly even if the full generation takes longer. It provides examples of different stream events and how to handle them in code."
image: "../../../assets/images/0b4c268-cohere_meta_image.jpg"  
keywords: "streaming, generative AI, text generation"

createdAt: "Thu Jun 01 2023 16:44:31 GMT+0000 (Coordinated Universal Time)"
updatedAt: "Tue Jun 18 2024 07:41:22 GMT+0000 (Coordinated Universal Time)"
---
The [Chat API](/reference/chat) is capable of streaming events (such as text generation) as they come. This means that partial results from the model can be displayed within moments, even if the full generation takes longer.

You're likely already familiar with streaming. When you ask the model a question using the [Coral](https://coral.cohere.com/) UI, the interface doesn't output a single block of text, instead it _streams_ the text out a few words at a time. In many user interfaces enabling streaming improves the user experience by lowering the perceived latency.

## Stream Events

When streaming is enabled, the API sends events down one by one. Each event has a `type`. Events of different types need to be handled correctly.

The following is an example of printing the `content-delta` event type from a streamed response, which contains the text contents of an LLM's response.

```python PYTHON
import cohere

co = cohere.ClientV2(api_key='<YOUR API KEY>')

res = co.chat_stream(model="command-r-plus",
                     messages=[{"role": "user", "content": "What is an LLM?"}])

for event in res:
    if event:
        if event.type == "content-delta":
            print(event.delta.message.content.text, end="")

```

```
# Sample output (streamed)

A large language model (LLM) is a type of artificial neural network model that has been trained on massive amounts of text data ...

```

The following sections describe the different types of events that are emitted during a streaming session.

### Basic Chat Stream Events

#### message-start

The first event in the stream containing metadata for the request such as the `id`. Only one `message-start` event will be emitted.

#### content-start

The event that indicates the start of the content block of the message. Only one `content-start` event will be emitted.

#### content-delta

The event that is emitted whenever the next chunk of text comes back from the model. As the model continues generating text, multiple events of this type will be emitted. Each event generates one token through the `delta.message.content.text` field.

```
# Sample events

index=0 delta=ChatContentDeltaEventDelta(message=ChatContentDeltaEventDeltaMessage(content=ChatContentDeltaEventDeltaMessageContent(text='A'))) type='content-delta'

index=0 delta=ChatContentDeltaEventDelta(message=ChatContentDeltaEventDeltaMessage(content=ChatContentDeltaEventDeltaMessageContent(text=' large'))) type='content-delta'

index=0 delta=ChatContentDeltaEventDelta(message=ChatContentDeltaEventDeltaMessage(content=ChatContentDeltaEventDeltaMessageContent(text=' language'))) type='content-delta'

...

```


#### content-end

The event that indicates the end of the content block of the message. Only one `content-end` event will be emitted.

#### message-end

The final event in the stream indicating the end of the streamed response. Only one `message-end` event will be emitted.

### Retrieval Augmented Generation Stream Events

#### message-start

Same as in a basic chat stream event.

#### content-start

Same as in a basic chat stream event.

#### content-delta

Same as in a basic chat stream event.

#### citation-start

Emitted for every citation generated in the response.

```
# Sample event

id=None delta=ChatMessageStartEventDelta(message=ChatMessageStartEventDeltaMessage(role=None, citations={'start': 14, 'end': 29, 'text': 'gym memberships', 'sources': [{'type': 'document', 'id': 'doc:0:1', 'document': {'id': 'doc:0:1', 'text': 'Health and Wellness Benefits: We care about your well-being and offer gym memberships, on-site yoga classes, and comprehensive health insurance.'}}]})) type='citation-start' index=0

```

#### citation-end

Emitted to indicate the end of a citation. If there are multiple citations generated, the events will come as a sequence of `citation-start` and `citation-end` pairs.

#### content-end

Same as in a basic chat stream event.

#### message-end

Same as in a basic chat stream event.

### Tool Use Stream Events (For Tool Calling)

#### message-start

Same as in a basic chat stream event.

#### tool-plan-delta

Emitted when the next token of the tool plan is generated.

```
# Sample events

delta=ChatToolPlanDeltaEventDelta(tool_plan=None, message={'tool_plan': 'I'}) type='tool-plan-delta'

delta=ChatToolPlanDeltaEventDelta(tool_plan=None, message={'tool_plan': ' will'}) type='tool-plan-delta'

delta=ChatToolPlanDeltaEventDelta(tool_plan=None, message={'tool_plan': ' use'}) type='tool-plan-delta'

...

```

#### tool-call-start

Emitted when the model generates tool calls that require actioning upon. The event contains a list of `tool_calls` containing the tool name and tool call ID of the tool.

```
# Sample event

index=0 delta=ChatToolCallStartEventDelta(tool_call=None, message={'tool_calls': {'id': 'get_weather_5zq8yjheb99p', 'type': 'function', 'function': {'name': 'get_weather', 'arguments': ''}}}) type='tool-call-start'

```
#### tool-call-delta

Emitted when the next token of the the tool call is generated.

```
# Sample events

index=0 delta=ChatToolCallDeltaEventDelta(tool_call=None, message={'tool_calls': {'function': {'arguments': '{\n    "'}}}) type='tool-call-delta'

index=0 delta=ChatToolCallDeltaEventDelta(tool_call=None, message={'tool_calls': {'function': {'arguments': 'location'}}}) type='tool-call-delta'

index=0 delta=ChatToolCallDeltaEventDelta(tool_call=None, message={'tool_calls': {'function': {'arguments': '":'}}}) type='tool-call-delta'

...
```

#### tool-call-end

Emitted when the tool call is finished.

#### message-end

Same as in a basic chat stream event.

### Tool Use Stream Events (For Response Generation)

#### message-start

Same as in a basic chat stream event.

#### content-start

Same as in a basic chat stream event.

#### content-delta

Same as in a basic chat stream event.

#### citation-start

Emitted for every citation generated in the response.

```
# Sample event

citations={'start': 5, 'end': 9, 'text': '20°C', 'sources': [{'type': 'tool', 'id': 'get_weather_9cyzpajy3ry0:0', 'tool_output': {'temperature': '20C'}}]})) type='citation-start'

```

#### citation-end

Emitted to indicate the end of a citation. If there are multiple citations generated, the events will come as a sequence of `citation-start` and `citation-end` pairs.

#### content-end

Same as in a basic chat stream event.

#### message-end

Same as in a basic chat stream event.

## Example Responses

Below, we have a stream of events which shows the **full** output you might see during a streaming session (this example being a scenario of tool use response generation):

```
# Sample events

id='4e818026-4051-4ac4-85e6-84fceffa9cf0' delta=ChatMessageStartEventDelta(message=ChatMessageStartEventDeltaMessage(role='assistant', content=[], tool_plan='', tool_calls=[], citations=[])) type='message-start' 

index=0 delta=ChatContentStartEventDelta(message=ChatContentStartEventDeltaMessage(content=ChatContentStartEventDeltaMessageContent(text='', type='text'))) type='content-start' 

index=0 delta=ChatContentDeltaEventDelta(message=ChatContentDeltaEventDeltaMessage(content=ChatContentDeltaEventDeltaMessageContent(text='It'))) type='content-delta' 

index=0 delta=ChatContentDeltaEventDelta(message=ChatContentDeltaEventDeltaMessage(content=ChatContentDeltaEventDeltaMessageContent(text="'s"))) type='content-delta' 

index=0 delta=ChatContentDeltaEventDelta(message=ChatContentDeltaEventDeltaMessage(content=ChatContentDeltaEventDeltaMessageContent(text=' 2'))) type='content-delta' 

index=0 delta=ChatContentDeltaEventDelta(message=ChatContentDeltaEventDeltaMessage(content=ChatContentDeltaEventDeltaMessageContent(text='0'))) type='content-delta' 

index=0 delta=ChatContentDeltaEventDelta(message=ChatContentDeltaEventDeltaMessage(content=ChatContentDeltaEventDeltaMessageContent(text='°'))) type='content-delta' 

index=0 delta=ChatContentDeltaEventDelta(message=ChatContentDeltaEventDeltaMessage(content=ChatContentDeltaEventDeltaMessageContent(text='C in'))) type='content-delta' 

index=0 delta=ChatContentDeltaEventDelta(message=ChatContentDeltaEventDeltaMessage(content=ChatContentDeltaEventDeltaMessageContent(text=' Toronto'))) type='content-delta' 

index=0 delta=ChatContentDeltaEventDelta(message=ChatContentDeltaEventDeltaMessage(content=ChatContentDeltaEventDeltaMessageContent(text='.'))) type='content-delta' 

id=None delta=ChatMessageStartEventDelta(message=ChatMessageStartEventDeltaMessage(role=None, citations={'start': 5, 'end': 9, 'text': '20°C', 'sources': [{'type': 'tool', 'id': 'get_weather_n34527zdzhxy:0', 'tool_output': {'temperature': '20C'}}]})) type='citation-start' index=0 

id=None delta=None type='citation-end' index=0 

index=0 type='content-end' 

id=None delta=ChatMessageEndEventDelta(finish_reason='COMPLETE', usage=Usage(billed_units=UsageBilledUnits(input_tokens=59.0, output_tokens=10.0, search_units=None, classifications=None), tokens=UsageTokens(input_tokens=977.0, output_tokens=52.0), api_version={'version': '2', 'is_experimental': True}, warnings=['You are using an experimental version, for more information please refer to https://docs.cohere.com/versioning-reference'])) type='message-end' 

```

It contains information about whether the streaming session is finished, what type of event is being fired, and the text that was generated by the model. 

Note that the citation objects in the response are returned as part of a RAG and tool use response, which you can learn more about in the [RAG](v2/docs/retrieval-augmented-generation-rag) and [tool use](v2/docs/tool-use) guides. 

When the model has finished generating, it returns the full text, some metadata, citations, and the documents that were used to ground the reply.
